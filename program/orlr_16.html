---
layout: paper
id: 16
slides_live_id: 38941557
rocket_id: orlr-paper-16
meeting_url: 
authors: "Haiping Wu, Khimya Khetarpal, and Doina Precup"
camera_ready: true
cmt_id: 16
kind: poster
session_id: 1
session_title: "Session TBD"
title: "Self-Supervised Attention-Aware Reinforcement Learning"
abstract: "Visual saliency has emerged as a major visualization tool for interpreting deep reinforcement learning (RL) agents. However, much of the existing research uses it as an analyzing tool rather than an inductive bias for policy learning. In this work, we use visual attention as an inductive bias for RL agents. We propose a novel self-supervised attention learning approach which could 1. learn to select regions of interest without explicit annotations, and 2. act as a plug for existing deep RL methods to improve the learning performance. We empirically show that the self-supervised attention-aware deep RL methods outperform the baselines in the context of both the rate of convergence and performance. Furthermore, the proposed self-supervised attention is not tied with specific policies, nor restricted to a specific scene. We posit that the proposed approach is a universal self-supervised attention module for multi-task learning and transfer learning, and empirically validate the generalization ability of the proposed method. Finally, we can also extract object keypoints as a byproduct of the learned attention. We demonstrate qualitatively that the extracted object keypoints are superior to existing methods."
track: research
live: true
video_file_url: none
youtube_url: none
---